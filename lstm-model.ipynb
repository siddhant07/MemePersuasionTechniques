{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7141258,"sourceType":"datasetVersion","datasetId":4121742}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Meme Persuation Technique Classifier","metadata":{}},{"cell_type":"markdown","source":"# Model Training Method 2: LSTM","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport tensorflow as tf\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\n\n# Your MultilabelBalancedRandomSampler class definition goes here\n\n# Function to load data from JSON file\ndef load_data(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    return data\n\n# Load train, validation, and dev data\ntrain_data = load_data(\"/kaggle/input/dataset/train.json\")\nval_data = load_data(\"/kaggle/input/dataset/validation.json\")\ndev_data = load_data(\"/kaggle/input/dataset/dev_unlabeled.json\")\n\n# Create DataFrames\ntrain_df = pd.DataFrame(train_data)\nval_df = pd.DataFrame(val_data)\ndev_df = pd.DataFrame(dev_data)\n\n# Process labels using MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nbinary_labels_train = mlb.fit_transform(train_df[\"labels\"])\nbinary_labels_val = mlb.transform(val_df[\"labels\"])\n\n# Preprocess text data\nmax_words = 10000\nmax_len = 100\n\n","metadata":{"id":"Xru8oPSeU2EU","execution":{"iopub.status.busy":"2023-12-14T20:04:24.156119Z","iopub.execute_input":"2023-12-14T20:04:24.156643Z","iopub.status.idle":"2023-12-14T20:04:24.304153Z","shell.execute_reply.started":"2023-12-14T20:04:24.156605Z","shell.execute_reply":"2023-12-14T20:04:24.302714Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\n\nfrom torch.utils.data.sampler import Sampler\n\n\nclass MultilabelBalancedRandomSampler(Sampler):\n    \"\"\"\n    MultilabelBalancedRandomSampler: This sampler operates on a multilabel dataset\n    comprising n_samples and n_classes. It selects samples from the data with equal\n    probability per class, thereby simultaneously oversampling minority classes and\n    undersampling majority classes. It is important to note that while using this \n    sampler does not ensure a uniform distribution of classes in the output samples, \n    it does guarantee that each class will have at least batch_size / n_classes samples\n    as batch_size approaches infinity.\n    \"\"\"\n\n    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n        self.labels = labels\n        self.indices = indices\n        if self.indices is None:\n            self.indices = range(len(labels))\n\n        self.num_classes = self.labels.shape[1]\n\n        # List of lists of example indices per class\n        self.class_indices = []\n        for class_ in range(self.num_classes):\n            lst = np.where(self.labels[:, class_] == 1)[0]\n            lst = lst[np.isin(lst, self.indices)]\n            self.class_indices.append(lst)\n\n        self.counts = [0] * self.num_classes\n\n        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n        self.class_choice = class_choice\n        self.current_class = 0\n\n    def __iter__(self):\n        self.count = 0\n        return self\n\n    def __next__(self):\n        if self.count >= len(self.indices):\n            raise StopIteration\n        self.count += 1\n        return self.sample()\n\n    def sample(self):\n        class_ = self.get_class()\n        class_indices = self.class_indices[class_]\n        chosen_index = np.random.choice(class_indices)\n        if self.class_choice == \"least_sampled\":\n            for class_, indicator in enumerate(self.labels[chosen_index]):\n                if indicator == 1:\n                    self.counts[class_] += 1\n        return chosen_index\n\n    def get_class(self):\n        if self.class_choice == \"random\":\n            class_ = random.randint(0, self.labels.shape[1] - 1)\n        elif self.class_choice == \"cycle\":\n            class_ = self.current_class\n            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n        elif self.class_choice == \"least_sampled\":\n            min_count = self.counts[0]\n            min_classes = [0]\n            for class_ in range(1, self.num_classes):\n                if self.counts[class_] < min_count:\n                    min_count = self.counts[class_]\n                    min_classes = [class_]\n                if self.counts[class_] == min_count:\n                    min_classes.append(class_)\n            class_ = np.random.choice(min_classes)\n        return class_\n\n    def __len__(self):\n        return len(self.indices)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T20:04:25.917918Z","iopub.execute_input":"2023-12-14T20:04:25.918385Z","iopub.status.idle":"2023-12-14T20:04:25.933085Z","shell.execute_reply.started":"2023-12-14T20:04:25.918343Z","shell.execute_reply":"2023-12-14T20:04:25.931804Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_df[\"text\"])\nX_train = tokenizer.texts_to_sequences(train_df[\"text\"])\nX_val = tokenizer.texts_to_sequences(val_df[\"text\"])\n\nX_train = pad_sequences(X_train, maxlen=max_len)\nX_val = pad_sequences(X_val, maxlen=max_len)\n\n# Create a tf.data.Dataset using MultilabelBalancedRandomSampler\ndef generator():\n    for sample in zip(X_train, binary_labels_train):\n        yield sample\n\nbatch_size = 50\nlabels_train = binary_labels_train\nsampler = MultilabelBalancedRandomSampler(labels_train)\n\ntrain_dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n    tf.TensorSpec(shape=(max_len,), dtype=tf.int32),\n    tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.float32)\n))\n\ntrain_dataset = train_dataset.shuffle(len(X_train)).batch(batch_size, drop_remainder=True)\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n\n\nfrom tensorflow.keras.layers import Dropout\n\n# Build LSTM model with modifications\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))  # Increase the number of LSTM units\nmodel.add(Dropout(0.5))  # Add dropout for regularization\nmodel.add(Bidirectional(LSTM(128)))\nmodel.add(Dropout(0.5))  # Add dropout for regularization\nmodel.add(Dense(len(mlb.classes_), activation='sigmoid'))\n\n# Compile the model\noptimizer = Adam(learning_rate=1e-3)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model using the balanced dataset\nepochs = 20\n\nhistory = model.fit(train_dataset, validation_data=(X_val, binary_labels_val), epochs=epochs, batch_size=batch_size)\n# Evaluate the model on the validation set\nval_preds = model.predict(X_val)\nval_preds_binary = (val_preds > 0.5).astype(int)\n\nval_accuracy = accuracy_score(binary_labels_val, val_preds_binary)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T20:04:27.263263Z","iopub.execute_input":"2023-12-14T20:04:27.264634Z","iopub.status.idle":"2023-12-14T20:27:49.230129Z","shell.execute_reply.started":"2023-12-14T20:04:27.264577Z","shell.execute_reply":"2023-12-14T20:27:49.229102Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/20\n140/140 [==============================] - 63s 391ms/step - loss: 0.2781 - accuracy: 0.1027 - val_loss: 0.2493 - val_accuracy: 0.1040\nEpoch 2/20\n140/140 [==============================] - 54s 375ms/step - loss: 0.2490 - accuracy: 0.1089 - val_loss: 0.2488 - val_accuracy: 0.1040\nEpoch 3/20\n140/140 [==============================] - 54s 378ms/step - loss: 0.2442 - accuracy: 0.1089 - val_loss: 0.2405 - val_accuracy: 0.1220\nEpoch 4/20\n140/140 [==============================] - 54s 380ms/step - loss: 0.2324 - accuracy: 0.1511 - val_loss: 0.2375 - val_accuracy: 0.1520\nEpoch 5/20\n140/140 [==============================] - 55s 381ms/step - loss: 0.2238 - accuracy: 0.1996 - val_loss: 0.2382 - val_accuracy: 0.1720\nEpoch 6/20\n140/140 [==============================] - 55s 382ms/step - loss: 0.2163 - accuracy: 0.2261 - val_loss: 0.2393 - val_accuracy: 0.1760\nEpoch 7/20\n140/140 [==============================] - 54s 378ms/step - loss: 0.2069 - accuracy: 0.2711 - val_loss: 0.2435 - val_accuracy: 0.1960\nEpoch 8/20\n140/140 [==============================] - 54s 380ms/step - loss: 0.1987 - accuracy: 0.2929 - val_loss: 0.2474 - val_accuracy: 0.2040\nEpoch 9/20\n140/140 [==============================] - 54s 382ms/step - loss: 0.1909 - accuracy: 0.3254 - val_loss: 0.2535 - val_accuracy: 0.1880\nEpoch 10/20\n140/140 [==============================] - 54s 380ms/step - loss: 0.1816 - accuracy: 0.3517 - val_loss: 0.2573 - val_accuracy: 0.1900\nEpoch 11/20\n140/140 [==============================] - 54s 378ms/step - loss: 0.1727 - accuracy: 0.3760 - val_loss: 0.2660 - val_accuracy: 0.1760\nEpoch 12/20\n140/140 [==============================] - 54s 379ms/step - loss: 0.1640 - accuracy: 0.3954 - val_loss: 0.2798 - val_accuracy: 0.1800\nEpoch 13/20\n140/140 [==============================] - 54s 380ms/step - loss: 0.1546 - accuracy: 0.4221 - val_loss: 0.2902 - val_accuracy: 0.1580\nEpoch 14/20\n140/140 [==============================] - 54s 381ms/step - loss: 0.1473 - accuracy: 0.4277 - val_loss: 0.3049 - val_accuracy: 0.1740\nEpoch 15/20\n140/140 [==============================] - 54s 381ms/step - loss: 0.1407 - accuracy: 0.4421 - val_loss: 0.3101 - val_accuracy: 0.1840\nEpoch 16/20\n140/140 [==============================] - 54s 381ms/step - loss: 0.1337 - accuracy: 0.4490 - val_loss: 0.3218 - val_accuracy: 0.1740\nEpoch 17/20\n140/140 [==============================] - 55s 382ms/step - loss: 0.1284 - accuracy: 0.4650 - val_loss: 0.3288 - val_accuracy: 0.1740\nEpoch 18/20\n140/140 [==============================] - 54s 377ms/step - loss: 0.1228 - accuracy: 0.4676 - val_loss: 0.3273 - val_accuracy: 0.1880\nEpoch 19/20\n140/140 [==============================] - 54s 378ms/step - loss: 0.1168 - accuracy: 0.4700 - val_loss: 0.3403 - val_accuracy: 0.1840\nEpoch 20/20\n140/140 [==============================] - 54s 378ms/step - loss: 0.1124 - accuracy: 0.4779 - val_loss: 0.3488 - val_accuracy: 0.1840\n16/16 [==============================] - 3s 101ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Classification Report on Validation Set:\")\nprint(classification_report(binary_labels_val, val_preds_binary, target_names=mlb.classes_))","metadata":{"execution":{"iopub.status.busy":"2023-12-14T20:31:47.925796Z","iopub.execute_input":"2023-12-14T20:31:47.926666Z","iopub.status.idle":"2023-12-14T20:31:47.951150Z","shell.execute_reply.started":"2023-12-14T20:31:47.926626Z","shell.execute_reply":"2023-12-14T20:31:47.950049Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Classification Report on Validation Set:\n                                                     precision    recall  f1-score   support\n\n                                Appeal to authority       0.50      0.29      0.36        63\n                           Appeal to fear/prejudice       0.00      0.00      0.00        27\n                                          Bandwagon       0.00      0.00      0.00         7\n               Black-and-white Fallacy/Dictatorship       0.12      0.04      0.06        53\n                          Causal Oversimplification       0.00      0.00      0.00        21\n                                              Doubt       0.10      0.04      0.06        24\n                          Exaggeration/Minimisation       0.00      0.00      0.00        27\n                                        Flag-waving       0.56      0.12      0.20        42\n                   Glittering generalities (Virtue)       0.33      0.11      0.17        36\n                                    Loaded Language       0.45      0.41      0.43       135\nMisrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         4\n                              Name calling/Labeling       0.50      0.32      0.39       116\n      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         2\n           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         4\n                               Reductio ad hitlerum       0.00      0.00      0.00         4\n                                         Repetition       1.00      0.04      0.08        23\n                                            Slogans       0.50      0.16      0.24        50\n                                             Smears       0.44      0.40      0.42       142\n                         Thought-terminating clich√©       0.20      0.05      0.08        38\n                                       Whataboutism       0.00      0.00      0.00        21\n\n                                          micro avg       0.43      0.23      0.30       839\n                                          macro avg       0.23      0.10      0.12       839\n                                       weighted avg       0.37      0.23      0.26       839\n                                        samples avg       0.26      0.18      0.20       839\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}